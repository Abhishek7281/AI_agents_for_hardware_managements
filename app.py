# import streamlit as st
# import pandas as pd
# from langchain.llms import HuggingFacePipeline
# from langchain.agents import create_pandas_dataframe_agent
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# # Load GPT-2 locally
# checkpoint = "gpt2"  # You can replace this with a better local model
# tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# model = AutoModelForCausalLM.from_pretrained(checkpoint)

# pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=100)
# llm = HuggingFacePipeline(pipeline=pipe)

# # --- Streamlit UI ---
# st.set_page_config(page_title="Local Hardware AI Agent", layout="centered")
# st.title("üß† Local GPT-2 Hardware Management Agent")
# st.markdown("Upload your hardware Excel and ask questions ‚Äî runs fully offline!")

# uploaded_file = st.file_uploader("üì§ Upload Excel File", type=["xlsx"])

# if uploaded_file:
#     try:
#         df = pd.read_excel(uploaded_file)
#         st.success("‚úÖ File uploaded successfully!")
#         st.dataframe(df.head())

#         # Use LangChain with local GPT-2
#         agent = create_pandas_dataframe_agent(llm, df, verbose=False)

#         query = st.text_input("üí¨ Ask a question about your hardware data:")
#         if query:
#             with st.spinner("Thinking..."):
#                 response = agent.run(query)
#                 st.write("ü§ñ", response)

#     except Exception as e:
#         st.error(f"‚ùå Error processing file: {e}")


# import streamlit as st
# import pandas as pd
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from langchain_community.llms import HuggingFacePipeline
# from langchain_experimental.agents import create_pandas_dataframe_agent

# # Load GPT-2 locally
# #checkpoint = "gpt2"
# checkpoint = "mistralai/Mistral-7B-Instruct-v0.2"
# tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# model = AutoModelForCausalLM.from_pretrained(checkpoint)
# pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=100)
# llm = HuggingFacePipeline(pipeline=pipe)

# # Streamlit UI
# st.set_page_config(page_title="Local Hardware AI Agent", layout="centered")
# st.title("üß† Local GPT-2 Hardware Management Agent")
# st.markdown("Upload your Excel hardware sheet and ask questions.")

# uploaded_file = st.file_uploader("üì§ Upload Excel File", type=["xlsx"])
# if uploaded_file:
#     try:
#         df = pd.read_excel(uploaded_file)
#         st.success("‚úÖ File uploaded successfully!")
#         st.dataframe(df.head())

#         # Use LangChain with GPT-2
#         agent = create_pandas_dataframe_agent(llm, df, verbose=False, allow_dangerous_code=True,
#     handle_parsing_errors=True)

#         query = st.text_input("üí¨ Ask a question about your hardware data:")
#         if query:
#             with st.spinner("Thinking..."):
#                 response = agent.run(query)
#                 st.write("ü§ñ", response)

#     except Exception as e:
#         st.error(f"‚ùå Error processing file: {e}")


# import streamlit as st
# import pandas as pd
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from langchain_community.llms import HuggingFacePipeline
# from langchain_experimental.agents import create_pandas_dataframe_agent

# # Set Streamlit page config
# st.set_page_config(page_title="Local Mistral Hardware AI Agent", layout="centered")
# st.title("üß† Local Mistral Hardware Management Agent")
# st.markdown("Upload your Excel hardware inventory and ask any question about it.")

# # Load Mistral model (make sure you have enough RAM/GPU)
# @st.cache_resource
# def load_mistral_model():
#     model_id = "mistralai/Mistral-7B-Instruct-v0.2"

#     tokenizer = AutoTokenizer.from_pretrained(model_id)
#     model = AutoModelForCausalLM.from_pretrained(
#         model_id,
#         torch_dtype=torch.float16,
#         device_map="auto"  # Auto-detect GPU or CPU
#     )

#     pipe = pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         max_new_tokens=256,
#         temperature=0,
#         repetition_penalty=1.1
#     )

#     return HuggingFacePipeline(pipeline=pipe)

# # Load model only once
# llm = load_mistral_model()

# # Upload Excel File
# uploaded_file = st.file_uploader("üì§ Upload Excel File", type=["xlsx"])

# if uploaded_file:
#     try:
#         df = pd.read_excel(uploaded_file)
#         st.success("‚úÖ File uploaded successfully!")
#         st.dataframe(df.head())

#         # Create LangChain agent with mistral
#         agent = create_pandas_dataframe_agent(
#             llm,
#             df,
#             verbose=False,
#             allow_dangerous_code=True,
#             handle_parsing_errors=True
#         )

#         query = st.text_input("üí¨ Ask a question about your hardware data:")
#         if query:
#             with st.spinner("Thinking..."):
#                 response = agent.run(query)
#                 st.write("ü§ñ", response)

#     except Exception as e:
#         st.error(f"‚ùå Error processing file: {e}")



import streamlit as st
import pandas as pd

from langchain_community.llms import Ollama
from langchain_experimental.agents import create_pandas_dataframe_agent

# --- Streamlit UI ---
st.set_page_config(layout="centered")
st.title("ü§ñ Local GPT-2-like Hardware Agent (Mistral via Ollama)")
st.markdown("Upload your Excel hardware sheet and ask questions about it.")

# --- File upload ---
uploaded_file = st.file_uploader("üì§ Upload Excel File", type=["xlsx"])
if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
        st.success("‚úÖ File uploaded successfully!")
        st.dataframe(df)

        # Load Mistral model via Ollama
        llm = Ollama(model="mistral")

        # Create agent
        agent = create_pandas_dataframe_agent(
            llm,
            df,
            verbose=True,
            handle_parsing_errors=True,
            agent_type="openai-tools",  # robust with tabular data
        )

        # User question
        user_query = st.text_input("üí¨ Ask a question about your hardware data:")
        if user_query:
            with st.spinner("üß† Thinking..."):
                try:
                    answer = agent.run(user_query)
                    st.success("üìå Answer:")
                    st.write(answer)
                except Exception as e:
                    st.error(f"‚ùå Error processing question: {e}")

    except Exception as e:
        st.error(f"‚ùå Error reading Excel file: {e}")

